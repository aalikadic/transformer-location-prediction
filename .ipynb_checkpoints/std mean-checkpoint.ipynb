{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071dd4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.header import Header\n",
    "from operator import index\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import scipy.spatial\n",
    "import scipy.io\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    dataset_folder,\n",
    "    dataset_name,\n",
    "    val_size,\n",
    "    gt,\n",
    "    horizon,\n",
    "    delim=\"\\t\",\n",
    "    train=True,\n",
    "    eval=False,\n",
    "    verbose=False,\n",
    "):\n",
    "\n",
    "    if train == True:\n",
    "        datasets_list = os.listdir(os.path.join(dataset_folder, dataset_name, \"train\"))\n",
    "        full_dt_folder = os.path.join(dataset_folder, dataset_name, \"train\")\n",
    "    if train == False and eval == False:\n",
    "        datasets_list = os.listdir(os.path.join(dataset_folder, dataset_name, \"val\"))\n",
    "        full_dt_folder = os.path.join(dataset_folder, dataset_name, \"val\")\n",
    "    if train == False and eval == True:\n",
    "        datasets_list = os.listdir(os.path.join(dataset_folder, dataset_name, \"test\"))\n",
    "        full_dt_folder = os.path.join(dataset_folder, dataset_name, \"test\")\n",
    "\n",
    "    datasets_list = datasets_list\n",
    "\n",
    "    data = {}\n",
    "    data_src = []\n",
    "    data_trg = []\n",
    "    data_seq_start = []\n",
    "    data_frames = []\n",
    "    data_dt = []\n",
    "    data_peds = []\n",
    "\n",
    "    val_src = []\n",
    "    val_trg = []\n",
    "    val_seq_start = []\n",
    "    val_frames = []\n",
    "    val_dt = []\n",
    "    val_peds = []\n",
    "    print(\"ovdje\")\n",
    "    if verbose:\n",
    "        print(\"start loading dataset\")\n",
    "        print(\"validation set size -> %i\" % (val_size))\n",
    "\n",
    "    for i_dt, dt in enumerate(datasets_list):\n",
    "        if verbose:\n",
    "            print(\"%03i / %03i - loading %s\" % (i_dt + 1, len(datasets_list), dt))\n",
    "        # if i_dt == 0 or i_dt == 1:\n",
    "        # data = pd.read_csv(os.path.join(full_dt_folder, dt), sep=\"\\t\", header=None)\n",
    "        # print(data.info())\n",
    "        # print(data.head())\n",
    "        raw_data = pd.read_csv(\n",
    "            os.path.join(full_dt_folder, dt),\n",
    "            delimiter=delim,\n",
    "            names=[\n",
    "                \"frame\",\n",
    "                \"ped\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"scale\",\n",
    "                \"head_x\",\n",
    "                \"head_y\",\n",
    "                \"height\",\n",
    "                \"l_ear_x\",\n",
    "                \"l_ear_y\",\n",
    "                \"l_elbow_x\",\n",
    "                \"l_elbow_y\",\n",
    "                \"l_eye_x\",\n",
    "                \"l_eye_y\",\n",
    "                \"l_foot_x\",\n",
    "                \"l_foot_y\",\n",
    "                \"l_hand_x\",\n",
    "                \"l_hand_y\",\n",
    "                \"l_hip_x\",\n",
    "                \"l_hip_y\",\n",
    "                \"l_knee_x\",\n",
    "                \"l_knee_y\",\n",
    "                \"l_shoulder_x\",\n",
    "                \"l_shoulder_y\",\n",
    "                \"neck_x\",\n",
    "                \"neck_y\",\n",
    "                \"r_ear_x\",\n",
    "                \"r_ear_y\",\n",
    "                \"r_elbow_x\",\n",
    "                \"r_elbow_y\",\n",
    "                \"r_eye_x\",\n",
    "                \"r_eye_y\",\n",
    "                \"r_foot_x\",\n",
    "                \"r_foot_y\",\n",
    "                \"r_hand_x\",\n",
    "                \"r_hand_y\",\n",
    "                \"r_hip_x\",\n",
    "                \"r_hip_y\",\n",
    "                \"r_knee_x\",\n",
    "                \"r_knee_y\",\n",
    "                \"r_shoulder_x\",\n",
    "                \"r_shoulder_y\",\n",
    "                \"threshold\",\n",
    "                \"width\",\n",
    "                \"R_1\",\n",
    "                \"R_2\",\n",
    "                \"R_3\",\n",
    "                \"T_1\",\n",
    "                \"T_2\",\n",
    "                \"T_3\",\n",
    "            ],\n",
    "            usecols=list(range(50)),\n",
    "            na_values=\"?\",\n",
    "        )\n",
    "\n",
    "        print(raw_data.columns)\n",
    "        raw_data.sort_values(by=[\"frame\", \"ped\"], inplace=True)\n",
    "\n",
    "        inp, out, info = get_strided_data_clust(raw_data, gt, horizon, 1)\n",
    "\n",
    "        dt_frames = info[\"frames\"]\n",
    "        dt_seq_start = info[\"seq_start\"]\n",
    "        dt_dataset = np.array([i_dt]).repeat(inp.shape[0])\n",
    "        dt_peds = info[\"peds\"]\n",
    "\n",
    "        if val_size > 0 and inp.shape[0] > val_size * 2.5:\n",
    "            if verbose:\n",
    "                print(\"created validation from %s\" % (dt))\n",
    "            k = random.sample(np.arange(inp.shape[0]).tolist(), val_size)\n",
    "            val_src.append(inp[k, :, :])\n",
    "            val_trg.append(out[k, :, :])\n",
    "            val_seq_start.append(dt_seq_start[k, :, :])\n",
    "            val_frames.append(dt_frames[k, :])\n",
    "            val_dt.append(dt_dataset[k])\n",
    "            val_peds.append(dt_peds[k])\n",
    "            inp = np.delete(inp, k, 0)\n",
    "            out = np.delete(out, k, 0)\n",
    "            dt_frames = np.delete(dt_frames, k, 0)\n",
    "            dt_seq_start = np.delete(dt_seq_start, k, 0)\n",
    "            dt_dataset = np.delete(dt_dataset, k, 0)\n",
    "            dt_peds = np.delete(dt_peds, k, 0)\n",
    "        elif val_size > 0:\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \"could not create validation from %s, size -> %i\"\n",
    "                    % (dt, inp.shape[0])\n",
    "                )\n",
    "\n",
    "        data_src.append(inp)\n",
    "        data_trg.append(out)\n",
    "        data_seq_start.append(dt_seq_start)\n",
    "        data_frames.append(dt_frames)\n",
    "        data_dt.append(dt_dataset)\n",
    "        data_peds.append(dt_peds)\n",
    "\n",
    "    data[\"src\"] = np.concatenate(data_src, 0)\n",
    "    data[\"trg\"] = np.concatenate(data_trg, 0)\n",
    "    data[\"seq_start\"] = np.concatenate(data_seq_start, 0)\n",
    "    data[\"frames\"] = np.concatenate(data_frames, 0)\n",
    "    data[\"dataset\"] = np.concatenate(data_dt, 0)\n",
    "    data[\"peds\"] = np.concatenate(data_peds, 0)\n",
    "    data[\"dataset_name\"] = datasets_list\n",
    "\n",
    "    mean = data[\"src\"].mean((0, 1))\n",
    "    std = data[\"src\"].std((0, 1))\n",
    "\n",
    "    if val_size > 0:\n",
    "        data_val = {}\n",
    "        data_val[\"src\"] = np.concatenate(val_src, 0)\n",
    "        data_val[\"trg\"] = np.concatenate(val_trg, 0)\n",
    "        data_val[\"seq_start\"] = np.concatenate(val_seq_start, 0)\n",
    "        data_val[\"frames\"] = np.concatenate(val_frames, 0)\n",
    "        data_val[\"dataset\"] = np.concatenate(val_dt, 0)\n",
    "        data_val[\"peds\"] = np.concatenate(val_peds, 0)\n",
    "\n",
    "        return IndividualTfDataset(data, \"train\", mean, std), IndividualTfDataset(\n",
    "            data_val, \"validation\", mean, std\n",
    "        )\n",
    "\n",
    "    return IndividualTfDataset(data, \"train\", mean, std), None\n",
    "\n",
    "    return IndividualTfDataset(data, \"train\", mean, std), IndividualTfDataset(\n",
    "        data_val, \"validation\", mean, std\n",
    "    )\n",
    "\n",
    "\n",
    "class IndividualTfDataset(Dataset):\n",
    "    def __init__(self, data, name, mean, std):\n",
    "        super(IndividualTfDataset, self).__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.name = name\n",
    "\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"src\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"src\": torch.Tensor(self.data[\"src\"][index]),\n",
    "            \"trg\": torch.Tensor(self.data[\"trg\"][index]),\n",
    "            \"frames\": self.data[\"frames\"][index],\n",
    "            \"seq_start\": self.data[\"seq_start\"][index],\n",
    "            \"dataset\": self.data[\"dataset\"][index],\n",
    "            \"peds\": self.data[\"peds\"][index],\n",
    "        }\n",
    "\n",
    "\n",
    "def create_folders(baseFolder, datasetName):\n",
    "    try:\n",
    "        os.mkdir(baseFolder)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        os.mkdir(os.path.join(baseFolder, datasetName))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_strided_data(dt, gt_size, horizon, step):\n",
    "    inp_te = []\n",
    "    dtt = dt.astype(np.float32)\n",
    "    raw_data = dtt\n",
    "\n",
    "    ped = raw_data.ped.unique()\n",
    "    frame = []\n",
    "    ped_ids = []\n",
    "    for p in ped:\n",
    "        for i in range(\n",
    "            1 + (raw_data[raw_data.ped == p].shape[0] - gt_size - horizon) // step\n",
    "        ):\n",
    "            frame.append(\n",
    "                dt[dt.ped == p]\n",
    "                .iloc[i * step : i * step + gt_size + horizon, [0]]\n",
    "                .values.squeeze()\n",
    "            )\n",
    "            # print(\"%i,%i,%i\" % (i * 4, i * 4 + gt_size, i * 4 + gt_size + horizon))\n",
    "            inp_te.append(\n",
    "                raw_data[raw_data.ped == p]\n",
    "                .iloc[i * step : i * step + gt_size + horizon, 2:4]\n",
    "                .values\n",
    "            )\n",
    "            ped_ids.append(p)\n",
    "\n",
    "    frames = np.stack(frame)\n",
    "    inp_te_np = np.stack(inp_te)\n",
    "    ped_ids = np.stack(ped_ids)\n",
    "\n",
    "    inp_no_start = inp_te_np[:, 1:, 0:2] - inp_te_np[:, :-1, 0:2]\n",
    "    inp_std = inp_no_start.std(axis=(0, 1))\n",
    "    inp_mean = inp_no_start.mean(axis=(0, 1))\n",
    "    inp_norm = inp_no_start\n",
    "    # inp_norm = (inp_no_start - inp_mean) / inp_std\n",
    "\n",
    "    # vis=inp_te_np[:,1:,2:4]/np.linalg.norm(inp_te_np[:,1:,2:4],2,axis=2)[:,:,np.newaxis]\n",
    "    # inp_norm=np.concatenate((inp_norm,vis),2)\n",
    "\n",
    "    return (\n",
    "        inp_norm[:, : gt_size - 1],\n",
    "        inp_norm[:, gt_size - 1 :],\n",
    "        {\n",
    "            \"mean\": inp_mean,\n",
    "            \"std\": inp_std,\n",
    "            \"seq_start\": inp_te_np[:, 0:1, :].copy(),\n",
    "            \"frames\": frames,\n",
    "            \"peds\": ped_ids,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def get_strided_data_2(dt, gt_size, horizon, step):\n",
    "    inp_te = []\n",
    "    dtt = dt.astype(np.float32)\n",
    "    raw_data = dtt\n",
    "\n",
    "    ped = raw_data.ped.unique()\n",
    "    frame = []\n",
    "    ped_ids = []\n",
    "    for p in ped:\n",
    "        for i in range(\n",
    "            1 + (raw_data[raw_data.ped == p].shape[0] - gt_size - horizon) // step\n",
    "        ):\n",
    "            frame.append(\n",
    "                dt[dt.ped == p]\n",
    "                .iloc[i * step : i * step + gt_size + horizon, [0]]\n",
    "                .values.squeeze()\n",
    "            )\n",
    "            # print(\"%i,%i,%i\" % (i * 4, i * 4 + gt_size, i * 4 + gt_size + horizon))\n",
    "            inp_te.append(\n",
    "                raw_data[raw_data.ped == p]\n",
    "                .iloc[i * step : i * step + gt_size + horizon, 2:50]\n",
    "                .values\n",
    "            )\n",
    "            ped_ids.append(p)\n",
    "\n",
    "    frames = np.stack(frame)\n",
    "    inp_te_np = np.stack(inp_te)\n",
    "    ped_ids = np.stack(ped_ids)\n",
    "\n",
    "    inp_relative_pos = inp_te_np - inp_te_np[:, :1, :]\n",
    "    inp_speed = np.concatenate(\n",
    "        (\n",
    "            np.zeros((inp_te_np.shape[0], 1, 2)),\n",
    "            inp_te_np[:, 1:, 0:2] - inp_te_np[:, :-1, 0:2],\n",
    "        ),\n",
    "        1,\n",
    "    )\n",
    "    inp_accel = np.concatenate(\n",
    "        (\n",
    "            np.zeros((inp_te_np.shape[0], 1, 2)),\n",
    "            inp_speed[:, 1:, 0:2] - inp_speed[:, :-1, 0:2],\n",
    "        ),\n",
    "        1,\n",
    "    )\n",
    "    # inp_std = inp_no_start.std(axis=(0, 1))\n",
    "    # inp_mean = inp_no_start.mean(axis=(0, 1))\n",
    "    # inp_norm= inp_no_start\n",
    "    # inp_norm = (inp_no_start - inp_mean) / inp_std\n",
    "\n",
    "    # vis=inp_te_np[:,1:,2:4]/np.linalg.norm(inp_te_np[:,1:,2:4],2,axis=2)[:,:,np.newaxis]\n",
    "    # inp_norm=np.concatenate((inp_norm,vis),2)\n",
    "    inp_norm = np.concatenate((inp_te_np, inp_relative_pos, inp_speed, inp_accel), 2)\n",
    "    inp_mean = np.zeros(8)\n",
    "    inp_std = np.ones(8)\n",
    "\n",
    "    return (\n",
    "        inp_norm[:, :gt_size],\n",
    "        inp_norm[:, gt_size:],\n",
    "        {\n",
    "            \"mean\": inp_mean,\n",
    "            \"std\": inp_std,\n",
    "            \"seq_start\": inp_te_np[:, 0:1, :].copy(),\n",
    "            \"frames\": frames,\n",
    "            \"peds\": ped_ids,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def get_strided_data_clust(dt, gt_size, horizon, step):\n",
    "    inp_te = []\n",
    "    dtt = dt.astype(np.float32)\n",
    "    raw_data = dtt\n",
    "\n",
    "    ped = raw_data.ped.unique()\n",
    "    # print(ped)\n",
    "    frame = []\n",
    "    ped_ids = []\n",
    "    # print(dt)\n",
    "    # print(gt_size)\n",
    "    # print(horizon)\n",
    "    # print(step)\n",
    "    for p in ped:\n",
    "        # print(f\"p is  {p}\")\n",
    "        # print(\"u p in ped\")\n",
    "        # print(raw_data[raw_data.ped == p].shape)\n",
    "\n",
    "        # print(1 + (raw_data[raw_data.ped == p].shape[0] - gt_size - horizon) // step)\n",
    "\n",
    "        for i in range(\n",
    "            1 + (raw_data[raw_data.ped == p].shape[0] - gt_size - horizon) // step\n",
    "        ):\n",
    "\n",
    "            frame.append(\n",
    "                dt[dt.ped == p]\n",
    "                .iloc[i * step : i * step + gt_size + horizon, [0]]\n",
    "                .values.squeeze()\n",
    "            )\n",
    "            # print(\"%i,%i,%i\" % (i * 4, i * 4 + gt_size, i * 4 + gt_size + horizon))\n",
    "            inp_te.append(\n",
    "                raw_data[raw_data.ped == p]\n",
    "                .iloc[i * step : i * step + gt_size + horizon, 2:50]\n",
    "                .values\n",
    "            )\n",
    "            ped_ids.append(p)\n",
    "\n",
    "    frames = np.stack(frame)\n",
    "    inp_te_np = np.stack(inp_te)\n",
    "    ped_ids = np.stack(ped_ids)\n",
    "\n",
    "    # inp_relative_pos= inp_te_np-inp_te_np[:,:1,:]\n",
    "    inp_speed = np.concatenate(\n",
    "        (\n",
    "            np.zeros((inp_te_np.shape[0], 1, 2)),\n",
    "            inp_te_np[:, 1:, 0:2] - inp_te_np[:, :-1, 0:2],\n",
    "        ),\n",
    "        1,\n",
    "    )\n",
    "    # inp_accel = np.concatenate((np.zeros((inp_te_np.shape[0],1,2)),inp_speed[:,1:,0:2] - inp_speed[:, :-1, 0:2]),1)\n",
    "    # inp_std = inp_no_start.std(axis=(0, 1))\n",
    "    # inp_mean = inp_no_start.mean(axis=(0, 1))\n",
    "    # inp_norm= inp_no_start\n",
    "    # inp_norm = (inp_no_start - inp_mean) / inp_std\n",
    "\n",
    "    # vis=inp_te_np[:,1:,2:4]/np.linalg.norm(inp_te_np[:,1:,2:4],2,axis=2)[:,:,np.newaxis]\n",
    "    # inp_norm=np.concatenate((inp_norm,vis),2)\n",
    "    inp_norm = np.concatenate((inp_te_np, inp_speed), 2)\n",
    "    inp_mean = np.zeros(4)\n",
    "    inp_std = np.ones(4)\n",
    "\n",
    "    return (\n",
    "        inp_norm[:, :gt_size],\n",
    "        inp_norm[:, gt_size:],\n",
    "        {\n",
    "            \"mean\": inp_mean,\n",
    "            \"std\": inp_std,\n",
    "            \"seq_start\": inp_te_np[:, 0:1, :].copy(),\n",
    "            \"frames\": frames,\n",
    "            \"peds\": ped_ids,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def distance_metrics(gt, preds):\n",
    "    errors = np.zeros(gt.shape[:-1])\n",
    "    print(\"errors shape\")\n",
    "    print(errors.shape)\n",
    "    print(\"preds shape\")\n",
    "    print(preds.shape)\n",
    "    print(\"gt shape\")\n",
    "    print(gt.shape)\n",
    "    preds = preds[:, :, :2]\n",
    "    print(\"preds shape\")\n",
    "    print(preds.shape)\n",
    "    for i in range(errors.shape[0]):\n",
    "        for j in range(errors.shape[1]):\n",
    "            errors[i, j] = scipy.spatial.distance.euclidean(gt[i, j], preds[i, j])\n",
    "    return errors.mean(), errors[:, -1].mean(), errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1026d327",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'fpl\\\\fpl\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12624/3158844861.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train_dataset, _ = create_dataset(\n\u001b[0m\u001b[0;32m      2\u001b[0m             \u001b[1;34m\"fpl\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m             \u001b[1;34m\"fpl\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12624/2044270382.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(dataset_folder, dataset_name, val_size, gt, horizon, delim, train, eval, verbose)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mdatasets_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mfull_dt_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0meval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'fpl\\\\fpl\\\\train'"
     ]
    }
   ],
   "source": [
    "train_dataset, _ = create_dataset(\n",
    "            \"datasets\",\n",
    "            \"fpl\",\n",
    "            0,\n",
    "            12,\n",
    "            8,\n",
    "            delim=\"\\t\",\n",
    "            train=True,\n",
    "            verbose=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c48668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
