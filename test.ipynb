{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11fa4ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.header import Header\n",
    "from operator import index\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import scipy.spatial\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "493920ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b58a53eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strided_data_clust(dt, gt_size, horizon, step):\n",
    "    inp_te = []\n",
    "    dtt = dt.astype(np.float32)\n",
    "    raw_data = dtt\n",
    "\n",
    "    ped = raw_data.ped.unique()\n",
    "    # print(ped)\n",
    "    frame = []\n",
    "    ped_ids = []\n",
    "    # print(dt)\n",
    "    # print(gt_size)\n",
    "    # print(horizon)\n",
    "    # print(step)\n",
    "    for p in ped:\n",
    "        # print(f\"p is  {p}\")\n",
    "        # print(\"u p in ped\")\n",
    "        # print(raw_data[raw_data.ped == p].shape)\n",
    "\n",
    "        # print(1 + (raw_data[raw_data.ped == p].shape[0] - gt_size - horizon) // step)\n",
    "\n",
    "        for i in range(\n",
    "            1 + (raw_data[raw_data.ped == p].shape[0] - gt_size - horizon) // step\n",
    "        ):\n",
    "\n",
    "            frame.append(\n",
    "                dt[dt.ped == p]\n",
    "                .iloc[i * step : i * step + gt_size + horizon, [0]]\n",
    "                .values.squeeze()\n",
    "            )\n",
    "            \n",
    "            # print(\"%i,%i,%i\" % (i * 4, i * 4 + gt_size, i * 4 + gt_size + horizon))\n",
    "            inp_te.append(\n",
    "                raw_data[raw_data.ped == p]\n",
    "                .iloc[i * step : i * step + gt_size + horizon, 2:6]\n",
    "                .values\n",
    "            )\n",
    "            ped_ids.append(p)\n",
    "\n",
    "    frames = np.stack(frame)\n",
    "    inp_te_np = np.stack(inp_te)\n",
    "    ped_ids = np.stack(ped_ids)\n",
    "\n",
    "    # inp_relative_pos= inp_te_np-inp_te_np[:,:1,:]\n",
    "    inp_speed = np.concatenate(\n",
    "        (\n",
    "            np.zeros((inp_te_np.shape[0], 1, 2)),\n",
    "            inp_te_np[:, 1:, 0:2] - inp_te_np[:, :-1, 0:2],\n",
    "        ),\n",
    "        1,\n",
    "    )\n",
    "    # inp_accel = np.concatenate((np.zeros((inp_te_np.shape[0],1,2)),inp_speed[:,1:,0:2] - inp_speed[:, :-1, 0:2]),1)\n",
    "    # inp_std = inp_no_start.std(axis=(0, 1))\n",
    "    # inp_mean = inp_no_start.mean(axis=(0, 1))\n",
    "    # inp_norm= inp_no_start\n",
    "    # inp_norm = (inp_no_start - inp_mean) / inp_std\n",
    "\n",
    "    # vis=inp_te_np[:,1:,2:4]/np.linalg.norm(inp_te_np[:,1:,2:4],2,axis=2)[:,:,np.newaxis]\n",
    "    # inp_norm=np.concatenate((inp_norm,vis),2)\n",
    "    inp_norm = np.concatenate((inp_te_np, inp_speed), 2)\n",
    "    inp_mean = np.zeros(4)\n",
    "    inp_std = np.ones(4)\n",
    "\n",
    "    return (\n",
    "        inp_norm[:, :gt_size],\n",
    "        inp_norm[:, gt_size:],\n",
    "        {\n",
    "            \"mean\": inp_mean,\n",
    "            \"std\": inp_std,\n",
    "            \"seq_start\": inp_te_np[:, 0:1, :].copy(),\n",
    "            \"frames\": frames,\n",
    "            \"peds\": ped_ids,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee196096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(\n",
    "    dataset_folder,\n",
    "    dataset_name,\n",
    "    val_size,\n",
    "    gt,\n",
    "    horizon,\n",
    "    delim=\"\\t\",\n",
    "    train=True,\n",
    "    eval=False,\n",
    "    verbose=False,\n",
    "):\n",
    "\n",
    "    if train == True:\n",
    "        datasets_list = os.listdir(os.path.join(dataset_folder, dataset_name, \"train\"))\n",
    "        full_dt_folder = os.path.join(dataset_folder, dataset_name, \"train\")\n",
    "    if train == False and eval == False:\n",
    "        datasets_list = os.listdir(os.path.join(dataset_folder, dataset_name, \"val\"))\n",
    "        full_dt_folder = os.path.join(dataset_folder, dataset_name, \"val\")\n",
    "    if train == False and eval == True:\n",
    "        datasets_list = os.listdir(os.path.join(dataset_folder, dataset_name, \"test\"))\n",
    "        full_dt_folder = os.path.join(dataset_folder, dataset_name, \"test\")\n",
    "\n",
    "    datasets_list = datasets_list\n",
    "    print(datasets_list)\n",
    "    data = {}\n",
    "    data_src = []\n",
    "    data_trg = []\n",
    "    data_seq_start = []\n",
    "    data_frames = []\n",
    "    data_dt = []\n",
    "    data_peds = []\n",
    "\n",
    "    val_src = []\n",
    "    val_trg = []\n",
    "    val_seq_start = []\n",
    "    val_frames = []\n",
    "    val_dt = []\n",
    "    val_peds = []\n",
    "    print(\"ovdje\")\n",
    "    if verbose:\n",
    "        print(\"start loading dataset\")\n",
    "        print(\"validation set size -> %i\" % (val_size))\n",
    "\n",
    "    for i_dt, dt in enumerate(datasets_list):\n",
    "        if verbose:\n",
    "            print(\"%03i / %03i - loading %s\" % (i_dt + 1, len(datasets_list), dt))\n",
    "        # if i_dt == 0 or i_dt == 1:\n",
    "        # data = pd.read_csv(os.path.join(full_dt_folder, dt), sep=\"\\t\", header=None)\n",
    "        # print(data.info())\n",
    "        # print(data.head())\n",
    "        raw_data = pd.read_csv(\n",
    "            os.path.join(full_dt_folder, dt),\n",
    "            delimiter=delim,\n",
    "            names=[\"frame\", \"ped\", \"x\", \"y\", \"test_1\", \"test_2\"],\n",
    "            usecols=[0, 1, 2, 3, 4, 5],\n",
    "            na_values=\"?\",\n",
    "        )\n",
    "\n",
    "        #print(raw_data.head())\n",
    "        raw_data.sort_values(by=[\"frame\", \"ped\"], inplace=True)\n",
    "        \n",
    "        inp, out, info = get_strided_data_clust(raw_data, gt, horizon, 1)\n",
    "        #print(inp, out, info)\n",
    "        dt_frames = info[\"frames\"]\n",
    "        dt_seq_start = info[\"seq_start\"]\n",
    "        dt_dataset = np.array([i_dt]).repeat(inp.shape[0])\n",
    "        dt_peds = info[\"peds\"]\n",
    "        print(inp)\n",
    "\n",
    "        if val_size > 0 and inp.shape[0] > val_size * 2.5:\n",
    "            if verbose:\n",
    "                print(\"created validation from %s\" % (dt))\n",
    "            k = random.sample(np.arange(inp.shape[0]).tolist(), val_size)\n",
    "            val_src.append(inp[k, :, :])\n",
    "            val_trg.append(out[k, :, :])\n",
    "            val_seq_start.append(dt_seq_start[k, :, :])\n",
    "            val_frames.append(dt_frames[k, :])\n",
    "            val_dt.append(dt_dataset[k])\n",
    "            val_peds.append(dt_peds[k])\n",
    "            inp = np.delete(inp, k, 0)\n",
    "            out = np.delete(out, k, 0)\n",
    "            dt_frames = np.delete(dt_frames, k, 0)\n",
    "            dt_seq_start = np.delete(dt_seq_start, k, 0)\n",
    "            dt_dataset = np.delete(dt_dataset, k, 0)\n",
    "            dt_peds = np.delete(dt_peds, k, 0)\n",
    "        elif val_size > 0:\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \"could not create validation from %s, size -> %i\"\n",
    "                    % (dt, inp.shape[0])\n",
    "                )\n",
    "\n",
    "        data_src.append(inp)\n",
    "        data_trg.append(out)\n",
    "        data_seq_start.append(dt_seq_start)\n",
    "        data_frames.append(dt_frames)\n",
    "        data_dt.append(dt_dataset)\n",
    "        data_peds.append(dt_peds)\n",
    "\n",
    "    data[\"src\"] = np.concatenate(data_src, 0)\n",
    "    data[\"trg\"] = np.concatenate(data_trg, 0)\n",
    "    data[\"seq_start\"] = np.concatenate(data_seq_start, 0)\n",
    "    data[\"frames\"] = np.concatenate(data_frames, 0)\n",
    "    data[\"dataset\"] = np.concatenate(data_dt, 0)\n",
    "    data[\"peds\"] = np.concatenate(data_peds, 0)\n",
    "    data[\"dataset_name\"] = datasets_list\n",
    "\n",
    "    mean = data[\"src\"].mean((0, 1))\n",
    "    std = data[\"src\"].std((0, 1))\n",
    "\n",
    "    if val_size > 0:\n",
    "        data_val = {}\n",
    "        data_val[\"src\"] = np.concatenate(val_src, 0)\n",
    "        data_val[\"trg\"] = np.concatenate(val_trg, 0)\n",
    "        data_val[\"seq_start\"] = np.concatenate(val_seq_start, 0)\n",
    "        data_val[\"frames\"] = np.concatenate(val_frames, 0)\n",
    "        data_val[\"dataset\"] = np.concatenate(val_dt, 0)\n",
    "        data_val[\"peds\"] = np.concatenate(val_peds, 0)\n",
    "\n",
    "        return IndividualTfDataset(data, \"train\", mean, std), IndividualTfDataset(\n",
    "            data_val, \"validation\", mean, std\n",
    "        )\n",
    "\n",
    "    return IndividualTfDataset(data, \"train\", mean, std), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11b376eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GOPR0236U20.txt']\n",
      "ovdje\n",
      "[[[ 7.15606506e+02  6.16524475e+02  1.00000001e-01  1.00000001e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 7.31325012e+02  6.26952026e+02  1.09999999e-01  1.09999999e-01\n",
      "    1.57185059e+01  1.04275513e+01]\n",
      "  [ 7.46811523e+02  6.29562500e+02  1.19999997e-01  1.19999997e-01\n",
      "    1.54865112e+01  2.61047363e+00]\n",
      "  ...\n",
      "  [ 7.22078003e+02  6.49086487e+02  1.40000001e-01  1.40000001e-01\n",
      "   -1.04660034e+01  1.28100586e+00]\n",
      "  [ 7.14283508e+02  6.51716980e+02  1.50000006e-01  1.50000006e-01\n",
      "   -7.79449463e+00  2.63049316e+00]\n",
      "  [ 7.15597473e+02  6.57002502e+02  1.59999996e-01  1.59999996e-01\n",
      "    1.31396484e+00  5.28552246e+00]]\n",
      "\n",
      " [[ 7.31325012e+02  6.26952026e+02  1.09999999e-01  1.09999999e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 7.46811523e+02  6.29562500e+02  1.19999997e-01  1.19999997e-01\n",
      "    1.54865112e+01  2.61047363e+00]\n",
      "  [ 7.45564514e+02  6.46489014e+02  1.19999997e-01  1.19999997e-01\n",
      "   -1.24700928e+00  1.69265137e+01]\n",
      "  ...\n",
      "  [ 7.14283508e+02  6.51716980e+02  1.50000006e-01  1.50000006e-01\n",
      "   -7.79449463e+00  2.63049316e+00]\n",
      "  [ 7.15597473e+02  6.57002502e+02  1.59999996e-01  1.59999996e-01\n",
      "    1.31396484e+00  5.28552246e+00]\n",
      "  [ 7.16922974e+02  6.73893494e+02  1.70000002e-01  1.70000002e-01\n",
      "    1.32550049e+00  1.68909912e+01]]\n",
      "\n",
      " [[ 7.46811523e+02  6.29562500e+02  1.19999997e-01  1.19999997e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 7.45564514e+02  6.46489014e+02  1.19999997e-01  1.19999997e-01\n",
      "   -1.24700928e+00  1.69265137e+01]\n",
      "  [ 7.32544006e+02  6.47805481e+02  1.29999995e-01  1.29999995e-01\n",
      "   -1.30205078e+01  1.31646729e+00]\n",
      "  ...\n",
      "  [ 7.15597473e+02  6.57002502e+02  1.59999996e-01  1.59999996e-01\n",
      "    1.31396484e+00  5.28552246e+00]\n",
      "  [ 7.16922974e+02  6.73893494e+02  1.70000002e-01  1.70000002e-01\n",
      "    1.32550049e+00  1.68909912e+01]\n",
      "  [ 7.28690491e+02  6.82981018e+02  1.80000007e-01  1.80000007e-01\n",
      "    1.17675171e+01  9.08752441e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 8.00369995e+02  5.31825989e+02  1.00000001e-01  1.00000001e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 8.21109985e+02  5.34418030e+02  1.00000001e-01  1.00000001e-01\n",
      "    2.07399902e+01  2.59204102e+00]\n",
      "  [ 8.32960510e+02  5.31867493e+02  1.00000001e-01  1.00000001e-01\n",
      "    1.18505249e+01 -2.55053711e+00]\n",
      "  ...\n",
      "  [ 8.05476501e+02  5.38337524e+02  1.00000001e-01  1.00000001e-01\n",
      "   -1.56500244e+01  1.28649902e+00]\n",
      "  [ 8.01595520e+02  5.43533020e+02  1.00000001e-01  1.00000001e-01\n",
      "   -3.88098145e+00  5.19549561e+00]\n",
      "  [ 8.01624023e+02  5.51322998e+02  1.00000001e-01  1.00000001e-01\n",
      "    2.85034180e-02  7.78997803e+00]]\n",
      "\n",
      " [[ 8.21109985e+02  5.34418030e+02  1.00000001e-01  1.00000001e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 8.32960510e+02  5.31867493e+02  1.00000001e-01  1.00000001e-01\n",
      "    1.18505249e+01 -2.55053711e+00]\n",
      "  [ 8.30338989e+02  5.34395996e+02  1.00000001e-01  1.00000001e-01\n",
      "   -2.62152100e+00  2.52850342e+00]\n",
      "  ...\n",
      "  [ 8.01595520e+02  5.43533020e+02  1.00000001e-01  1.00000001e-01\n",
      "   -3.88098145e+00  5.19549561e+00]\n",
      "  [ 8.01624023e+02  5.51322998e+02  1.00000001e-01  1.00000001e-01\n",
      "    2.85034180e-02  7.78997803e+00]\n",
      "  [ 8.05495972e+02  5.54003479e+02  1.00000001e-01  1.00000001e-01\n",
      "    3.87194824e+00  2.68048096e+00]]\n",
      "\n",
      " [[ 8.32960510e+02  5.31867493e+02  1.00000001e-01  1.00000001e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 8.30338989e+02  5.34395996e+02  1.00000001e-01  1.00000001e-01\n",
      "   -2.62152100e+00  2.52850342e+00]\n",
      "  [ 8.21126526e+02  5.37051025e+02  1.00000001e-01  1.00000001e-01\n",
      "   -9.21246338e+00  2.65502930e+00]\n",
      "  ...\n",
      "  [ 8.01624023e+02  5.51322998e+02  1.00000001e-01  1.00000001e-01\n",
      "    2.85034180e-02  7.78997803e+00]\n",
      "  [ 8.05495972e+02  5.54003479e+02  1.00000001e-01  1.00000001e-01\n",
      "    3.87194824e+00  2.68048096e+00]\n",
      "  [ 8.14706482e+02  5.70867981e+02  1.00000001e-01  1.00000001e-01\n",
      "    9.21051025e+00  1.68645020e+01]]]\n"
     ]
    }
   ],
   "source": [
    "a, _ = create_dataset(\n",
    "            \"datasets\",\n",
    "            \"fpl\",\n",
    "            val_size=0,\n",
    "            gt=8,\n",
    "            horizon=12,\n",
    "            delim=\"\\t\",\n",
    "            train=True,\n",
    "            verbose=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9beeefe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': tensor([[ 7.1561e+02,  6.1652e+02,  1.0000e-01,  1.0000e-01,  0.0000e+00,\n",
       "           0.0000e+00],\n",
       "         [ 7.3133e+02,  6.2695e+02,  1.1000e-01,  1.1000e-01,  1.5719e+01,\n",
       "           1.0428e+01],\n",
       "         [ 7.4681e+02,  6.2956e+02,  1.2000e-01,  1.2000e-01,  1.5487e+01,\n",
       "           2.6105e+00],\n",
       "         [ 7.4556e+02,  6.4649e+02,  1.2000e-01,  1.2000e-01, -1.2470e+00,\n",
       "           1.6927e+01],\n",
       "         [ 7.3254e+02,  6.4781e+02,  1.3000e-01,  1.3000e-01, -1.3021e+01,\n",
       "           1.3165e+00],\n",
       "         [ 7.2208e+02,  6.4909e+02,  1.4000e-01,  1.4000e-01, -1.0466e+01,\n",
       "           1.2810e+00],\n",
       "         [ 7.1428e+02,  6.5172e+02,  1.5000e-01,  1.5000e-01, -7.7945e+00,\n",
       "           2.6305e+00],\n",
       "         [ 7.1560e+02,  6.5700e+02,  1.6000e-01,  1.6000e-01,  1.3140e+00,\n",
       "           5.2855e+00]]),\n",
       " 'trg': tensor([[ 7.1692e+02,  6.7389e+02,  1.7000e-01,  1.7000e-01,  1.3255e+00,\n",
       "           1.6891e+01],\n",
       "         [ 7.2869e+02,  6.8298e+02,  1.8000e-01,  1.8000e-01,  1.1768e+01,\n",
       "           9.0875e+00],\n",
       "         [ 7.3256e+02,  6.8298e+02,  1.9000e-01,  1.9000e-01,  3.8695e+00,\n",
       "           0.0000e+00],\n",
       "         [ 7.3258e+02,  6.7653e+02,  1.9500e-01,  1.9500e-01,  1.6479e-02,\n",
       "          -6.4530e+00],\n",
       "         [ 7.2873e+02,  6.7518e+02,  1.0000e-01,  1.0000e-01, -3.8445e+00,\n",
       "          -1.3440e+00],\n",
       "         [ 7.2859e+02,  6.7518e+02,  1.0000e-01,  1.0000e-01, -1.4197e-01,\n",
       "          -3.5400e-03],\n",
       "         [ 7.2338e+02,  6.6738e+02,  1.0000e-01,  1.0000e-01, -5.2141e+00,\n",
       "          -7.8030e+00],\n",
       "         [ 7.1823e+02,  6.5692e+02,  1.0000e-01,  1.0000e-01, -5.1495e+00,\n",
       "          -1.0458e+01],\n",
       "         [ 7.2477e+02,  6.5041e+02,  1.0000e-01,  1.0000e-01,  6.5460e+00,\n",
       "          -6.5050e+00],\n",
       "         [ 7.3520e+02,  6.3744e+02,  1.0000e-01,  1.0000e-01,  1.0428e+01,\n",
       "          -1.2974e+01],\n",
       "         [ 7.4819e+02,  6.3348e+02,  1.0000e-01,  1.0000e-01,  1.2990e+01,\n",
       "          -3.9600e+00],\n",
       "         [ 7.5597e+02,  6.3348e+02,  1.0000e-01,  1.0000e-01,  7.7745e+00,\n",
       "           0.0000e+00]]),\n",
       " 'frames': array([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "        20, 21, 22], dtype=int64),\n",
       " 'seq_start': array([[7.156065e+02, 6.165245e+02, 1.000000e-01, 1.000000e-01]],\n",
       "       dtype=float32),\n",
       " 'dataset': 0,\n",
       " 'peds': 0.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08882fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2fd8f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndividualTfDataset(Dataset):\n",
    "    def __init__(self, data, name, mean, std):\n",
    "        super(IndividualTfDataset, self).__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.name = name\n",
    "\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"src\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"src\": torch.Tensor(self.data[\"src\"][index]),\n",
    "            \"trg\": torch.Tensor(self.data[\"trg\"][index]),\n",
    "            \"frames\": self.data[\"frames\"][index],\n",
    "            \"seq_start\": self.data[\"seq_start\"][index],\n",
    "            \"dataset\": self.data[\"dataset\"][index],\n",
    "            \"peds\": self.data[\"peds\"][index],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3525e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strided_data(dt, gt_size, horizon, step):\n",
    "    inp_te = []\n",
    "    dtt = dt.astype(np.float32)\n",
    "    raw_data = dtt\n",
    "\n",
    "    ped = raw_data.ped.unique()\n",
    "    frame = []\n",
    "    ped_ids = []\n",
    "    for p in ped:\n",
    "        for i in range(\n",
    "            1 + (raw_data[raw_data.ped == p].shape[0] - gt_size - horizon) // step\n",
    "        ):\n",
    "            frame.append(\n",
    "                dt[dt.ped == p]\n",
    "                .iloc[i * step : i * step + gt_size + horizon, [0]]\n",
    "                .values.squeeze()\n",
    "            )\n",
    "            # print(\"%i,%i,%i\" % (i * 4, i * 4 + gt_size, i * 4 + gt_size + horizon))\n",
    "            inp_te.append(\n",
    "                raw_data[raw_data.ped == p]\n",
    "                .iloc[i * step : i * step + gt_size + horizon, 2:4]\n",
    "                .values\n",
    "            )\n",
    "            ped_ids.append(p)\n",
    "\n",
    "    frames = np.stack(frame)\n",
    "    inp_te_np = np.stack(inp_te)\n",
    "    ped_ids = np.stack(ped_ids)\n",
    "\n",
    "    inp_no_start = inp_te_np[:, 1:, 0:2] - inp_te_np[:, :-1, 0:2]\n",
    "    inp_std = inp_no_start.std(axis=(0, 1))\n",
    "    inp_mean = inp_no_start.mean(axis=(0, 1))\n",
    "    inp_norm = inp_no_start\n",
    "    # inp_norm = (inp_no_start - inp_mean) / inp_std\n",
    "\n",
    "    # vis=inp_te_np[:,1:,2:4]/np.linalg.norm(inp_te_np[:,1:,2:4],2,axis=2)[:,:,np.newaxis]\n",
    "    # inp_norm=np.concatenate((inp_norm,vis),2)\n",
    "\n",
    "    return (\n",
    "        inp_norm[:, : gt_size - 1],\n",
    "        inp_norm[:, gt_size - 1 :],\n",
    "        {\n",
    "            \"mean\": inp_mean,\n",
    "            \"std\": inp_std,\n",
    "            \"seq_start\": inp_te_np[:, 0:1, :].copy(),\n",
    "            \"frames\": frames,\n",
    "            \"peds\": ped_ids,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bc1777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strided_data_2(dt, gt_size, horizon, step):\n",
    "    inp_te = []\n",
    "    dtt = dt.astype(np.float32)\n",
    "    raw_data = dtt\n",
    "\n",
    "    ped = raw_data.ped.unique()\n",
    "    frame = []\n",
    "    ped_ids = []\n",
    "    for p in ped:\n",
    "        for i in range(\n",
    "            1 + (raw_data[raw_data.ped == p].shape[0] - gt_size - horizon) // step\n",
    "        ):\n",
    "            frame.append(\n",
    "                dt[dt.ped == p]\n",
    "                .iloc[i * step : i * step + gt_size + horizon, [0]]\n",
    "                .values.squeeze()\n",
    "            )\n",
    "            # print(\"%i,%i,%i\" % (i * 4, i * 4 + gt_size, i * 4 + gt_size + horizon))\n",
    "            inp_te.append(\n",
    "                raw_data[raw_data.ped == p]\n",
    "                .iloc[i * step : i * step + gt_size + horizon, 2:4]\n",
    "                .values\n",
    "            )\n",
    "            ped_ids.append(p)\n",
    "\n",
    "    frames = np.stack(frame)\n",
    "    inp_te_np = np.stack(inp_te)\n",
    "    ped_ids = np.stack(ped_ids)\n",
    "\n",
    "    inp_relative_pos = inp_te_np - inp_te_np[:, :1, :]\n",
    "    inp_speed = np.concatenate(\n",
    "        (\n",
    "            np.zeros((inp_te_np.shape[0], 1, 2)),\n",
    "            inp_te_np[:, 1:, 0:2] - inp_te_np[:, :-1, 0:2],\n",
    "        ),\n",
    "        1,\n",
    "    )\n",
    "    inp_accel = np.concatenate(\n",
    "        (\n",
    "            np.zeros((inp_te_np.shape[0], 1, 2)),\n",
    "            inp_speed[:, 1:, 0:2] - inp_speed[:, :-1, 0:2],\n",
    "        ),\n",
    "        1,\n",
    "    )\n",
    "    # inp_std = inp_no_start.std(axis=(0, 1))\n",
    "    # inp_mean = inp_no_start.mean(axis=(0, 1))\n",
    "    # inp_norm= inp_no_start\n",
    "    # inp_norm = (inp_no_start - inp_mean) / inp_std\n",
    "\n",
    "    # vis=inp_te_np[:,1:,2:4]/np.linalg.norm(inp_te_np[:,1:,2:4],2,axis=2)[:,:,np.newaxis]\n",
    "    # inp_norm=np.concatenate((inp_norm,vis),2)\n",
    "    inp_norm = np.concatenate((inp_te_np, inp_relative_pos, inp_speed, inp_accel), 2)\n",
    "    inp_mean = np.zeros(8)\n",
    "    inp_std = np.ones(8)\n",
    "\n",
    "    return (\n",
    "        inp_norm[:, :gt_size],\n",
    "        inp_norm[:, gt_size:],\n",
    "        {\n",
    "            \"mean\": inp_mean,\n",
    "            \"std\": inp_std,\n",
    "            \"seq_start\": inp_te_np[:, 0:1, :].copy(),\n",
    "            \"frames\": frames,\n",
    "            \"peds\": ped_ids,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba1ecc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a6a36b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_metrics(gt, preds):\n",
    "    errors = np.zeros(preds.shape[:-1])\n",
    "    for i in range(errors.shape[0]):\n",
    "        for j in range(errors.shape[1]):\n",
    "            errors[i, j] = scipy.spatial.distance.euclidean(gt[i, j], preds[i, j])\n",
    "    return errors.mean(), errors[:, -1].mean(), errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ec4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformer.decoder import Decoder\n",
    "from transformer.multihead_attention import MultiHeadAttention\n",
    "from transformer.positional_encoding import PositionalEncoding\n",
    "from transformer.pointerwise_feedforward import PointerwiseFeedforward\n",
    "from transformer.encoder_decoder import EncoderDecoder\n",
    "from transformer.encoder import Encoder\n",
    "from transformer.encoder_layer import EncoderLayer\n",
    "from transformer.decoder_layer import DecoderLayer\n",
    "from transformer.batch import subsequent_mask\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "\n",
    "import copy\n",
    "import math\n",
    "\n",
    "class IndividualTF(nn.Module):\n",
    "    def __init__(self, enc_inp_size, dec_inp_size, dec_out_size, N=6,\n",
    "                   d_model=512, d_ff=2048, h=8, dropout=0.1,mean=[0,0],std=[0,0]):\n",
    "        super(IndividualTF, self).__init__()\n",
    "        \"Helper: Construct a model from hyperparameters.\"\n",
    "        c = copy.deepcopy\n",
    "        attn = MultiHeadAttention(h, d_model)\n",
    "        ff = PointerwiseFeedforward(d_model, d_ff, dropout)\n",
    "        position = PositionalEncoding(d_model, dropout)\n",
    "        self.mean=np.array(mean)\n",
    "        self.std=np.array(std)\n",
    "        self.model = EncoderDecoder(\n",
    "            Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "            Decoder(DecoderLayer(d_model, c(attn), c(attn),\n",
    "                                 c(ff), dropout), N),\n",
    "            nn.Sequential(LinearEmbedding(enc_inp_size,d_model), c(position)),\n",
    "            nn.Sequential(LinearEmbedding(dec_inp_size,d_model), c(position)),\n",
    "            Generator(d_model, dec_out_size))\n",
    "\n",
    "        # This was important from their code.\n",
    "        # Initialize parameters with Glorot / fan_avg.\n",
    "        for p in self.model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, *input):\n",
    "        return self.model.generator(self.model(*input))\n",
    "\n",
    "class LinearEmbedding(nn.Module):\n",
    "    def __init__(self, inp_size,d_model):\n",
    "        super(LinearEmbedding, self).__init__()\n",
    "        # lut => lookup table\n",
    "        self.lut = nn.Linear(inp_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "\n",
    "    def __init__(self, d_model, out_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41ae877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GOPR0236U20.txt', 'GOPR0238U20.txt', 'GOPR0241U20.txt', 'GOPR0242U20.txt', 'GOPR0245U20.txt']\n",
      "ovdje\n",
      "   frame  ped         x         y\n",
      "0      3    0  715.6065  616.5245\n",
      "1      4    0  731.3250  626.9520\n",
      "2      5    0  746.8115  629.5625\n",
      "3      6    0  745.5645  646.4890\n",
      "4      7    0  732.5440  647.8055\n",
      "     frame  ped         x         y\n",
      "0        3    0  715.6065  616.5245\n",
      "332      3   10  551.4005  563.0890\n",
      "379      3   20  800.3700  531.8260\n",
      "1        4    0  731.3250  626.9520\n",
      "333      4   10  567.0265  570.9215\n",
      "   frame  ped         x         y\n",
      "0     33   20  591.8165  464.0145\n",
      "1     34   20  581.2315  460.1445\n",
      "2     35   20  578.7150  470.6395\n",
      "3     36   20  560.5225  482.2710\n",
      "4     37   20  542.2580  470.6605\n",
      "   frame  ped         x         y\n",
      "0     33   20  591.8165  464.0145\n",
      "1     34   20  581.2315  460.1445\n",
      "2     35   20  578.7150  470.6395\n",
      "3     36   20  560.5225  482.2710\n",
      "4     37   20  542.2580  470.6605\n",
      "   frame  ped         x         y\n",
      "0      6  140  812.1210  651.6835\n",
      "1      7  140  774.2185  634.7635\n",
      "2      8  140  746.8580  616.5765\n",
      "3      9  140  727.3720  612.6920\n",
      "4     10  140  703.8740  594.4370\n",
      "    frame  ped         x         y\n",
      "0       6  140  812.1210  651.6835\n",
      "1       7  140  774.2185  634.7635\n",
      "29      7  170  951.5550  513.5585\n",
      "2       8  140  746.8580  616.5765\n",
      "30      8  170  950.1695  504.4070\n",
      "   frame  ped         x         y\n",
      "0      3   10  370.2090  448.4510\n",
      "1      4   10  372.7140  449.7255\n",
      "2      5   10  388.3815  449.7295\n",
      "3      6   10  402.7655  435.3465\n",
      "4      7   10  424.9150  428.9165\n",
      "    frame  ped         x         y\n",
      "0       3   10  370.2090  448.4510\n",
      "28      3   20  912.3455  445.7340\n",
      "1       4   10  372.7140  449.7255\n",
      "2       5   10  388.3815  449.7295\n",
      "29      5   20  920.3250  448.4920\n",
      "   frame  ped         x         y\n",
      "0      3    0  480.9150  565.7275\n",
      "1      4    0  480.9285  563.0665\n",
      "2      5    0  480.9960  559.2195\n",
      "3      6    0  481.0410  555.3135\n",
      "4      7    0  481.0545  555.2960\n",
      "     frame  ped         x         y\n",
      "0        3    0  480.9150  565.7275\n",
      "57       3   10  574.7970  581.3330\n",
      "117      3   20  748.1665  555.3280\n",
      "147      3   30  634.7765  547.3945\n",
      "226      3   40  937.1480  555.2815\n",
      "['fpl_val.txt']\n",
      "ovdje\n",
      "   frame  ped         x         y\n",
      "0     10    3  437.9760  470.5940\n",
      "1     10    4  479.6540  466.6515\n",
      "2     10    5  516.1745  449.7825\n",
      "3     10    6  542.2620  440.5830\n",
      "4     10    7  581.2835  443.1545\n",
      "      frame  ped         x         y\n",
      "22        0    3  248.9415  499.2180\n",
      "50        0    3  715.6065  616.5245\n",
      "930       0    3  480.9150  565.7275\n",
      "1433      0    3  388.4370  533.1255\n",
      "1450      0    3  651.7440  483.5585\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28908/1805263187.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     ).to(device)\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import baselineUtils\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import time\n",
    "from transformer.batch import subsequent_mask\n",
    "from torch.optim import Adam, SGD, RMSprop, Adagrad\n",
    "from transformer.noam_opt import NoamOpt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import individual_TF\n",
    "\n",
    "train_dataset, _ = create_dataset(\n",
    "            \"datasets\",\n",
    "            \"fpl\",\n",
    "            val_size=0,\n",
    "            gt=8,\n",
    "            horizon=12,\n",
    "            delim=\"\\t\",\n",
    "            train=True,\n",
    "            verbose=False\n",
    "        )\n",
    "val_dataset, _ = create_dataset(\n",
    "            \"datasets\",\n",
    "            \"fpl\",\n",
    "            val_size=0,\n",
    "            gt=8,\n",
    "            horizon=12,\n",
    "            delim=\"\\t\",\n",
    "            train=False,\n",
    "            verbose=False)\n",
    "\n",
    "   \n",
    "\n",
    "model = individual_TF.IndividualTF(\n",
    "        2,\n",
    "        3,\n",
    "        3,\n",
    "        N=6,\n",
    "        d_model=512,\n",
    "        d_ff=2048,\n",
    "        h=8,\n",
    "        dropout=0.1,\n",
    "        mean=[0, 0],\n",
    "        std=[0, 0],\n",
    "    ).to(device)\n",
    "    \n",
    "\n",
    "tr_dl = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=70, shuffle=True, num_workers=0\n",
    "    )\n",
    "\n",
    "    \n",
    "val_dl = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=70, shuffle=True, num_workers=0\n",
    "    )\n",
    "\n",
    "test_dl = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=70, shuffle=False, num_workers=0\n",
    "    )\n",
    "\n",
    "    # optim = SGD(list(a.parameters())+list(model.parameters())+list(generator.parameters()),lr=0.01)\n",
    "    # sched=torch.optim.lr_scheduler.StepLR(optim,0.0005)\n",
    "optim = NoamOpt(\n",
    "        512,\n",
    "        1,\n",
    "        len(tr_dl) * 10,\n",
    "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9),\n",
    "    )\n",
    "    # optim=Adagrad(list(a.parameters())+list(model.parameters())+list(generator.parameters()),lr=0.01,lr_decay=0.001)\n",
    "epoch = 0\n",
    "\n",
    "    # mean=train_dataset[:]['src'][:,1:,2:4].mean((0,1))\n",
    "mean = torch.cat(\n",
    "        (train_dataset[:][\"src\"][:, 1:, 2:4], train_dataset[:][\"trg\"][:, :, 2:4]), 1\n",
    "    ).mean((0, 1))\n",
    "    # std=train_dataset[:]['src'][:,1:,2:4].std((0,1))\n",
    "std = torch.cat(\n",
    "    (train_dataset[:][\"src\"][:, 1:, 2:4], train_dataset[:][\"trg\"][:, :, 2:4]), 1\n",
    ").std((0, 1))\n",
    "    \n",
    "means = []\n",
    "stds = []\n",
    "for i in np.unique(train_dataset[:][\"dataset\"]):\n",
    "    ind = train_dataset[:][\"dataset\"] == i\n",
    "    means.append(\n",
    "        torch.cat(\n",
    "            (\n",
    "                train_dataset[:][\"src\"][ind, 1:, 2:4],\n",
    "                train_dataset[:][\"trg\"][ind, :, 2:4],\n",
    "            ),\n",
    "            1,\n",
    "        ).mean((0, 1))\n",
    "    )\n",
    "    stds.append(\n",
    "        torch.cat(\n",
    "                (\n",
    "                    train_dataset[:][\"src\"][ind, 1:, 2:4],\n",
    "                    train_dataset[:][\"trg\"][ind, :, 2:4],\n",
    "                ),\n",
    "                1,\n",
    "            ).std((0, 1))\n",
    "        )\n",
    "mean = torch.stack(means).mean(0)\n",
    "std = torch.stack(stds).mean(0)\n",
    "\n",
    "\n",
    "max_epoch = 5\n",
    "while epoch < max_epoch:\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for id_b, batch in enumerate(tr_dl):\n",
    "\n",
    "        optim.optimizer.zero_grad()\n",
    "        inp = (batch[\"src\"][:, 1:, 2:4].to(device) - mean.to(device)) / std.to(\n",
    "            device\n",
    "        )\n",
    "        target = (batch[\"trg\"][:, :-1, 2:4].to(device) - mean.to(device)) / std.to(\n",
    "            device\n",
    "        )\n",
    "        target_c = torch.zeros((target.shape[0], target.shape[1], 1)).to(device)\n",
    "        target = torch.cat((target, target_c), -1)\n",
    "        start_of_seq = (\n",
    "            torch.Tensor([0, 0, 1])\n",
    "            .unsqueeze(0)\n",
    "            .unsqueeze(1)\n",
    "            .repeat(target.shape[0], 1, 1)\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        dec_inp = torch.cat((start_of_seq, target), 1)\n",
    "\n",
    "        src_att = torch.ones((inp.shape[0], 1, inp.shape[1])).to(device)\n",
    "        trg_att = (\n",
    "            subsequent_mask(dec_inp.shape[1])\n",
    "            .repeat(dec_inp.shape[0], 1, 1)\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        pred = model(inp, dec_inp, src_att, trg_att)\n",
    "\n",
    "        loss = (\n",
    "            F.pairwise_distance(\n",
    "                pred[:, :, 0:2].contiguous().view(-1, 2),\n",
    "                (\n",
    "                    (batch[\"trg\"][:, :, 2:4].to(device) - mean.to(device))\n",
    "                    / std.to(device)\n",
    "                )\n",
    "                .contiguous()\n",
    "                .view(-1, 2)\n",
    "                .to(device),\n",
    "            ).mean()\n",
    "            + torch.mean(torch.abs(pred[:, :, 2]))\n",
    "        )\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        # sched.step()\n",
    "    log.add_scalar(\"Loss/train\", epoch_loss / len(tr_dl), epoch)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = 0\n",
    "        step = 0\n",
    "        model.eval()\n",
    "        gt = []\n",
    "        pr = []\n",
    "        inp_ = []\n",
    "        peds = []\n",
    "        frames = []\n",
    "        dt = []\n",
    "\n",
    "        for id_b, batch in enumerate(val_dl):\n",
    "            inp_.append(batch[\"src\"])\n",
    "            gt.append(batch[\"trg\"][:, :, 0:2])\n",
    "            frames.append(batch[\"frames\"])\n",
    "            peds.append(batch[\"peds\"])\n",
    "            dt.append(batch[\"dataset\"])\n",
    "\n",
    "            inp = (batch[\"src\"][:, 1:, 2:4].to(device) - mean.to(device)) / std.to(\n",
    "                    device\n",
    "            )\n",
    "            src_att = torch.ones((inp.shape[0], 1, inp.shape[1])).to(device)\n",
    "            start_of_seq = (\n",
    "                torch.Tensor([0, 0, 1])\n",
    "                .unsqueeze(0)\n",
    "                .unsqueeze(1)\n",
    "                .repeat(inp.shape[0], 1, 1)\n",
    "                .to(device)\n",
    "            )\n",
    "            dec_inp = start_of_seq\n",
    "            \n",
    "            #preds\n",
    "            for i in range(12):\n",
    "                trg_att = (\n",
    "                    subsequent_mask(dec_inp.shape[1])\n",
    "                    .repeat(dec_inp.shape[0], 1, 1)\n",
    "                    .to(device)\n",
    "                )\n",
    "                out = model(inp, dec_inp, src_att, trg_att)\n",
    "                dec_inp = torch.cat((dec_inp, out[:, -1:, :]), 1)\n",
    "\n",
    "            preds_tr_b = (\n",
    "                dec_inp[:, 1:, 0:2] * std.to(device) + mean.to(device)\n",
    "            ).cpu().numpy().cumsum(1) + batch[\"src\"][:, -1:, 0:2].cpu().numpy()\n",
    "            pr.append(preds_tr_b)\n",
    "            \n",
    "\n",
    "        peds = np.concatenate(peds, 0)\n",
    "        frames = np.concatenate(frames, 0)\n",
    "        dt = np.concatenate(dt, 0)\n",
    "        gt = np.concatenate(gt, 0)\n",
    "        dt_names = test_dataset.data[\"dataset_name\"]\n",
    "        pr = np.concatenate(pr, 0)\n",
    "        mad, fad, errs = baselineUtils.distance_metrics(gt, pr)\n",
    "        log.add_scalar(\"validation/MAD\", mad, epoch)\n",
    "        log.add_scalar(\"validation/FAD\", fad, epoch)\n",
    "\n",
    "        if True:\n",
    "\n",
    "            model.eval()\n",
    "            gt = []\n",
    "            pr = []\n",
    "            inp_ = []\n",
    "            peds = []\n",
    "            frames = []\n",
    "            dt = []\n",
    "\n",
    "            for id_b, batch in enumerate(test_dl):\n",
    "                inp_.append(batch[\"src\"])\n",
    "                gt.append(batch[\"trg\"][:, :, 0:2])\n",
    "                frames.append(batch[\"frames\"])\n",
    "                peds.append(batch[\"peds\"])\n",
    "                dt.append(batch[\"dataset\"])\n",
    "\n",
    "                inp = (\n",
    "                    batch[\"src\"][:, 1:, 2:4].to(device) - mean.to(device)\n",
    "                ) / std.to(device)\n",
    "                src_att = torch.ones((inp.shape[0], 1, inp.shape[1])).to(device)\n",
    "                start_of_seq = (\n",
    "                    torch.Tensor([0, 0, 1])\n",
    "                    .unsqueeze(0)\n",
    "                    .unsqueeze(1)\n",
    "                    .repeat(inp.shape[0], 1, 1)\n",
    "                    .to(device)\n",
    "                )\n",
    "                dec_inp = start_of_seq\n",
    "                #preds\n",
    "                for i in range(12):\n",
    "                    trg_att = (\n",
    "                        subsequent_mask(dec_inp.shape[1])\n",
    "                        .repeat(dec_inp.shape[0], 1, 1)\n",
    "                        .to(device)\n",
    "                    )\n",
    "                    out = model(inp, dec_inp, src_att, trg_att)\n",
    "                    dec_inp = torch.cat((dec_inp, out[:, -1:, :]), 1)\n",
    "\n",
    "                preds_tr_b = (\n",
    "                    dec_inp[:, 1:, 0:2] * std.to(device) + mean.to(device)\n",
    "                ).cpu().numpy().cumsum(1) + batch[\"src\"][:, -1:, 0:2].cpu().numpy()\n",
    "                pr.append(preds_tr_b)\n",
    "                \n",
    "\n",
    "            peds = np.concatenate(peds, 0)\n",
    "            frames = np.concatenate(frames, 0)\n",
    "            dt = np.concatenate(dt, 0)\n",
    "            gt = np.concatenate(gt, 0)\n",
    "            dt_names = test_dataset.data[\"dataset_name\"]\n",
    "            pr = np.concatenate(pr, 0)\n",
    "            mad, fad, errs = baselineUtils.distance_metrics(gt, pr)\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "    epoch += 1\n",
    "ab = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a24d02af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GOPR0236U20.txt']\n",
      "ovdje\n",
      "   frame  ped         x         y\n",
      "0      3    0  715.6065  616.5245\n",
      "1      4    0  731.3250  626.9520\n",
      "2      5    0  746.8115  629.5625\n",
      "3      6    0  745.5645  646.4890\n",
      "4      7    0  732.5440  647.8055\n",
      "     frame  ped         x         y\n",
      "0        3    0  715.6065  616.5245\n",
      "332      3   10  551.4005  563.0890\n",
      "379      3   20  800.3700  531.8260\n",
      "1        4    0  731.3250  626.9520\n",
      "333      4   10  567.0265  570.9215\n"
     ]
    }
   ],
   "source": [
    "a, _ = create_dataset(\n",
    "            \"datasets\",\n",
    "            \"fpl\",\n",
    "            val_size=0,\n",
    "            gt=8,\n",
    "            horizon=12,\n",
    "            delim=\"\\t\",\n",
    "            train=True,\n",
    "            verbose=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9586cf8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': tensor([[715.6065, 616.5245,   0.0000,   0.0000],\n",
       "         [731.3250, 626.9520,  15.7185,  10.4276],\n",
       "         [746.8115, 629.5625,  15.4865,   2.6105],\n",
       "         [745.5645, 646.4890,  -1.2470,  16.9265],\n",
       "         [732.5440, 647.8055, -13.0205,   1.3165],\n",
       "         [722.0780, 649.0865, -10.4660,   1.2810],\n",
       "         [714.2835, 651.7170,  -7.7945,   2.6305],\n",
       "         [715.5975, 657.0025,   1.3140,   5.2855]]),\n",
       " 'trg': tensor([[ 7.1692e+02,  6.7389e+02,  1.3255e+00,  1.6891e+01],\n",
       "         [ 7.2869e+02,  6.8298e+02,  1.1768e+01,  9.0875e+00],\n",
       "         [ 7.3256e+02,  6.8298e+02,  3.8695e+00,  0.0000e+00],\n",
       "         [ 7.3258e+02,  6.7653e+02,  1.6479e-02, -6.4530e+00],\n",
       "         [ 7.2873e+02,  6.7518e+02, -3.8445e+00, -1.3440e+00],\n",
       "         [ 7.2859e+02,  6.7518e+02, -1.4197e-01, -3.5400e-03],\n",
       "         [ 7.2338e+02,  6.6738e+02, -5.2141e+00, -7.8030e+00],\n",
       "         [ 7.1823e+02,  6.5692e+02, -5.1495e+00, -1.0458e+01],\n",
       "         [ 7.2477e+02,  6.5041e+02,  6.5460e+00, -6.5050e+00],\n",
       "         [ 7.3520e+02,  6.3744e+02,  1.0428e+01, -1.2974e+01],\n",
       "         [ 7.4819e+02,  6.3348e+02,  1.2990e+01, -3.9600e+00],\n",
       "         [ 7.5597e+02,  6.3348e+02,  7.7745e+00,  0.0000e+00]]),\n",
       " 'frames': array([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "        20, 21, 22], dtype=int64),\n",
       " 'seq_start': array([[715.6065, 616.5245]], dtype=float32),\n",
       " 'dataset': 0,\n",
       " 'peds': 0.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b16d3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': tensor([[731.3250, 626.9520,   0.0000,   0.0000],\n",
       "         [746.8115, 629.5625,  15.4865,   2.6105],\n",
       "         [745.5645, 646.4890,  -1.2470,  16.9265],\n",
       "         [732.5440, 647.8055, -13.0205,   1.3165],\n",
       "         [722.0780, 649.0865, -10.4660,   1.2810],\n",
       "         [714.2835, 651.7170,  -7.7945,   2.6305],\n",
       "         [715.5975, 657.0025,   1.3140,   5.2855],\n",
       "         [716.9230, 673.8935,   1.3255,  16.8910]]),\n",
       " 'trg': tensor([[ 7.2869e+02,  6.8298e+02,  1.1768e+01,  9.0875e+00],\n",
       "         [ 7.3256e+02,  6.8298e+02,  3.8695e+00,  0.0000e+00],\n",
       "         [ 7.3258e+02,  6.7653e+02,  1.6479e-02, -6.4530e+00],\n",
       "         [ 7.2873e+02,  6.7518e+02, -3.8445e+00, -1.3440e+00],\n",
       "         [ 7.2859e+02,  6.7518e+02, -1.4197e-01, -3.5400e-03],\n",
       "         [ 7.2338e+02,  6.6738e+02, -5.2141e+00, -7.8030e+00],\n",
       "         [ 7.1823e+02,  6.5692e+02, -5.1495e+00, -1.0458e+01],\n",
       "         [ 7.2477e+02,  6.5041e+02,  6.5460e+00, -6.5050e+00],\n",
       "         [ 7.3520e+02,  6.3744e+02,  1.0428e+01, -1.2974e+01],\n",
       "         [ 7.4819e+02,  6.3348e+02,  1.2990e+01, -3.9600e+00],\n",
       "         [ 7.5597e+02,  6.3348e+02,  7.7745e+00,  0.0000e+00],\n",
       "         [ 7.6905e+02,  6.3215e+02,  1.3086e+01, -1.3345e+00]]),\n",
       " 'frames': array([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
       "        21, 22, 23], dtype=int64),\n",
       " 'seq_start': array([[731.325, 626.952]], dtype=float32),\n",
       " 'dataset': 0,\n",
       " 'peds': 0.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
